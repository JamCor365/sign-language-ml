{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93c7dd13",
   "metadata": {},
   "source": [
    "# Sistema Split‑and‑Merge para Alfabeto de Señas\n",
    "Combina **SVM + Landmarks MediaPipe** para letras estáticas y **CNN 3D (R3D‑18)** para letras dinámicas  \n",
    "*Generado: 2025-07-05*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64960e89",
   "metadata": {},
   "source": [
    "\n",
    "## Resumen del flujo\n",
    "\n",
    "1. **Detector de movimiento** con flujo óptico → decide si la mano está quieta o en movimiento.  \n",
    "2. **Letras estáticas** (mano quieta) → MediaPipe Hands → 63 landmarks → `StandardScaler` → **SVM RBF**.  \n",
    "3. **Letras dinámicas** (mano moviéndose) → acumulamos 16 frames 224×224 → **R3D‑18** en GPU.\n",
    "\n",
    "Arquivos de modelo que debes tener en la misma carpeta del notebook:\n",
    "\n",
    "| Archivo | Rol |\n",
    "|---------|-----|\n",
    "| `letters_landmarks_scaler.pkl` | Escalador para los vectores de 63 landmarks |\n",
    "| `letters_landmarks_svm.pkl`    | SVM entrenado sobre esos landmarks |\n",
    "| `r3d18_dynamic.pth`            | Pesos de la CNN 3D para las 5 letras dinámicas |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e63096b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch CUDA disponible: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cv2, joblib, time, collections, numpy as np\n",
    "import mediapipe as mp\n",
    "from pathlib import Path\n",
    "import torch, torch.nn as nn\n",
    "from torchvision.models.video import r3d_18\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Torch CUDA disponible:', torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "10646a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ─── Parámetros generales ────────────────────────────────────────\n",
    "CLIP_LEN = 16          # nº de frames para la CNN 3D\n",
    "VID_SIZE = 224         # tamaño cuadrado de entrada\n",
    "TAU      = 1.2         # umbral de energía de movimiento (ajústalo)\n",
    "LABELS_STATIC  = list(\"ABCDEFGHILMNOPRSTUVWY\")   # 21 letras estáticas\n",
    "LABELS_DYNAMIC = ['J', 'K', 'Q', 'X', 'Z', 'Ñ']\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5c20433",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jamin\\miniconda3\\envs\\signml\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Jamin\\miniconda3\\envs\\signml\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\Jamin\\AppData\\Local\\Temp\\ipykernel_13080\\1279762796.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  cnn.load_state_dict(torch.load('../models/r3d18_dynamic.pth', map_location=DEVICE))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelos cargados correctamente\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ─── Cargar modelos estáticos ───────────────────────────────────\n",
    "scaler = joblib.load('../models/letters_landmarks_scaler.pkl')\n",
    "svm    = joblib.load('../models/letters_landmarks_svm.pkl')\n",
    "\n",
    "# ─── Cargar modelo dinámico ─────────────────────────────────────\n",
    "cnn = r3d_18(pretrained=False)\n",
    "cnn.fc = nn.Linear(cnn.fc.in_features, len(LABELS_DYNAMIC))\n",
    "cnn.load_state_dict(torch.load('../models/r3d18_dynamic.pth', map_location=DEVICE))\n",
    "cnn = cnn.to(DEVICE).eval()\n",
    "\n",
    "print('Modelos cargados correctamente')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01863d50",
   "metadata": {},
   "source": [
    "### Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1e29809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "def motion_energy(frame_bgr, prev_gray):\n",
    "    gray = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n",
    "    if prev_gray is None:\n",
    "        return 0, gray\n",
    "    flow = cv2.calcOpticalFlowFarneback(prev_gray, gray,\n",
    "                                        None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
    "    return np.mean(np.linalg.norm(flow, axis=2)), gray\n",
    "\n",
    "def extract_landmarks(frame_bgr, detector):\n",
    "    img_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    res = detector.process(img_rgb)\n",
    "    if not res.multi_hand_landmarks:\n",
    "        return None\n",
    "    lm = res.multi_hand_landmarks[0]\n",
    "    coords = np.array([[p.x, p.y, p.z] for p in lm.landmark])\n",
    "    coords -= coords[0]\n",
    "    norm = np.linalg.norm(coords).mean()\n",
    "    coords /= norm if norm else 1\n",
    "    return coords.flatten()          # shape (63,)\n",
    "\n",
    "def classify_static(frame, detector):\n",
    "    feats = extract_landmarks(frame, detector)\n",
    "    if feats is None:                       # no mano detectada\n",
    "        return None, 0.0\n",
    "    feats_std = scaler.transform(feats.reshape(1, -1))\n",
    "    idx  = svm.predict(feats_std)[0]\n",
    "    conf = svm.decision_function(feats_std).max()\n",
    "    return idx, conf\n",
    "\n",
    "def classify_dynamic(clip):\n",
    "    tensor = np.stack([cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in clip])  # T,H,W,C\n",
    "    tensor = torch.from_numpy(tensor.astype('float32')/255.).permute(3,0,1,2)  # C,T,H,W\n",
    "    with torch.no_grad():\n",
    "        out  = cnn(tensor.unsqueeze(0).to(DEVICE))\n",
    "        idx  = out.argmax(1).item()\n",
    "        conf = torch.softmax(out, dim=1)[0, idx].item()\n",
    "    return LABELS_DYNAMIC[idx], conf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1dae05",
   "metadata": {},
   "source": [
    "\n",
    "### Bucle de demo en vivo  \n",
    "Ejecuta la siguiente celda para usar la webcam.  \n",
    "Pulsa **ESC** para salir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f3dc191",
   "metadata": {},
   "outputs": [],
   "source": [
    "with mp_hands.Hands(static_image_mode=False,\n",
    "                    max_num_hands=1,\n",
    "                    min_detection_confidence=0.5,\n",
    "                    min_tracking_confidence=0.5) as hands_detector:\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    clip_buf = collections.deque(maxlen=CLIP_LEN)\n",
    "    prev_gray, cooldown = None, 0\n",
    "\n",
    "    try:\n",
    "        while True:\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                break\n",
    "\n",
    "            energy, prev_gray = motion_energy(frame, prev_gray)\n",
    "            clip_buf.append(cv2.resize(frame, (VID_SIZE, VID_SIZE)))\n",
    "\n",
    "            text = ''\n",
    "            if energy < TAU and len(clip_buf):\n",
    "                letter, conf = classify_static(frame, hands_detector)\n",
    "                if letter:\n",
    "                    text = f'{letter} ({conf:.2f})'\n",
    "                    cooldown = 0\n",
    "            else:\n",
    "                if len(clip_buf) == CLIP_LEN and cooldown == 0:\n",
    "                    letter, conf = classify_dynamic(list(clip_buf))\n",
    "                    text = f'{letter} ({conf:.2f})'\n",
    "                    cooldown = CLIP_LEN\n",
    "                else:\n",
    "                    cooldown = max(0, cooldown-1)\n",
    "\n",
    "            cv2.putText(frame, f'E={energy:.2f}', (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 0), 2)\n",
    "            cv2.putText(frame, text, (10, 70),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1.3, (0, 255, 0), 3)\n",
    "            cv2.imshow('Split-Merge Sign Detector', frame)\n",
    "            if cv2.waitKey(1) & 0xFF == 27:   # ESC\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "signml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
