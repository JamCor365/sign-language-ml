# ü§ü Proyecto de Reconocimiento de Lengua de Se√±as en Espa√±ol (LSM)

Este proyecto permite reconocer letras y palabras de la Lengua de Se√±as Mexicana (LSM) a partir de im√°genes o video. Utiliza t√©cnicas de Machine Learning y visi√≥n computacional, con modelos entrenados sobre letras est√°ticas y din√°micas.

## üìÅ Estructura del proyecto

sign_language_project/
‚îÇ
‚îú‚îÄ‚îÄ data/ # Datasets de letras y gestos
‚îú‚îÄ‚îÄ notebooks/ # Notebooks de an√°lisis, entrenamiento y prueba
‚îú‚îÄ‚îÄ src/ # Scripts de procesamiento y modelos
‚îú‚îÄ‚îÄ models/ # Modelos entrenados (.pkl)
‚îú‚îÄ‚îÄ environment.yml # üü¢ Entorno Conda completo
‚îú‚îÄ‚îÄ requirements.txt # ‚ö™Ô∏è Requisitos para pip (opcional)
‚îî‚îÄ‚îÄ README.md

---

## ‚öôÔ∏è Requisitos

- Python **3.11.x**
- [Git](https://git-scm.com/)
- Opcional: [Miniconda](https://docs.conda.io/en/latest/miniconda.html)

---

## üß™ Instalaci√≥n del entorno

### üîπ Opci√≥n 1: con **Conda** (recomendado)

1. **Clona este repositorio**:

```bash
git clone https://github.com/JamCor365/sign-language-ml.git
cd sign_language_project
```

2. **Crea el entorno Conda**:

```bash
conda env create -f environment.yml
conda activate signml
```

3. **(Opcional) Si usas Jupyter**:

```bash
python -m ipykernel install --user --name signml --display-name "Python (signml)"
```

### üîπ Opci√≥n 2: con **pip** y entorno virtual de Python

1. **Crea entorno virtual manual**:

```bash
python3.11 -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate
```

2. **Instala dependencias desde `requirements.txt`**:

```bash
pip install -r requirements.txt
```
> Aseg√∫rate de tener Python 3.11.x instalado previamente.

---

## üöÄ Uso

Abre los notebooks desde `notebooks/` para:

* `EDA_letters.ipynb`: Exploraci√≥n y limpieza de letras
* `Training_letters.ipynb`: Entrenamiento de modelo est√°tico
* `EDA_dynamics.ipynb`: Exploraci√≥n y limpieza de secuencias din√°micas
* `Training_dynamics.ipynb`: Entrenamiento de modelo din√°mico
* `predict_from_video.ipynb`: Prueba del sistema con un video de entrada
* `realtime_inference.ipynb`: Prueba en vivo con c√°mara web

---

## üß† Modelos utilizados

* HOG + PCA + SVM / Random Forest para letras
* Promedio de frames + PCA + Random Forest para gestos din√°micos
* Exploraci√≥n con CNN + LSTM como modelo opcional (basado en [este art√≠culo](https://www.nature.com/articles/s41598-024-76174-7))

